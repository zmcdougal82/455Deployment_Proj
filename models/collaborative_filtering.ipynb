{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Recommendation Model\n",
    "\n",
    "This notebook builds a collaborative filtering recommendation model using the Surprise library and saves it to a .sav file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Collaborative Filtering Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFilteringModel:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.user_mapping = {}  # Maps original user IDs to internal IDs\n",
    "        self.item_mapping = {}  # Maps original item IDs to internal IDs\n",
    "        self.reverse_user_mapping = {}  # Maps internal IDs back to original user IDs\n",
    "        self.reverse_item_mapping = {}  # Maps internal IDs back to original item IDs\n",
    "        self.user_items = defaultdict(list)  # Items rated by each user\n",
    "        self.item_users = defaultdict(list)  # Users who rated each item\n",
    "        \n",
    "    def fit(self, interactions_df):\n",
    "        \"\"\"\n",
    "        Train the collaborative filtering model\n",
    "        \n",
    "        Parameters:\n",
    "        - interactions_df: DataFrame with columns [personId, contentId, eventType]\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "        \n",
    "        # Create user and item mappings\n",
    "        unique_users = interactions_df['personId'].unique()\n",
    "        unique_items = interactions_df['contentId'].unique()\n",
    "        \n",
    "        self.user_mapping = {user: i for i, user in enumerate(unique_users)}\n",
    "        self.item_mapping = {item: i for i, item in enumerate(unique_items)}\n",
    "        \n",
    "        self.reverse_user_mapping = {i: user for user, i in self.user_mapping.items()}\n",
    "        self.reverse_item_mapping = {i: item for item, i in self.item_mapping.items()}\n",
    "        \n",
    "        # Convert eventType to numerical ratings\n",
    "        # VIEW = 1, FOLLOW = 2, etc. (can be customized based on your data)\n",
    "        event_type_mapping = {\n",
    "            'VIEW': 1.0,\n",
    "            'FOLLOW': 2.0\n",
    "        }\n",
    "        \n",
    "        # Create a new DataFrame with user_id, item_id, and rating\n",
    "        ratings_data = []\n",
    "        for _, row in interactions_df.iterrows():\n",
    "            user_id = self.user_mapping[row['personId']]\n",
    "            item_id = self.item_mapping[row['contentId']]\n",
    "            event_type = row['eventType']\n",
    "            rating = event_type_mapping.get(event_type, 1.0)  # Default to 1.0 if event type not found\n",
    "            \n",
    "            ratings_data.append({\n",
    "                'user_id': user_id,\n",
    "                'item_id': item_id,\n",
    "                'rating': rating\n",
    "            })\n",
    "            \n",
    "            # Store user-item and item-user relationships\n",
    "            self.user_items[user_id].append(item_id)\n",
    "            self.item_users[item_id].append(user_id)\n",
    "        \n",
    "        ratings_df = pd.DataFrame(ratings_data)\n",
    "        \n",
    "        # Create a Surprise dataset\n",
    "        reader = Reader(rating_scale=(1, 2))\n",
    "        dataset = Dataset.load_from_df(ratings_df[['user_id', 'item_id', 'rating']], reader)\n",
    "        \n",
    "        # Build the full trainset\n",
    "        trainset = dataset.build_full_trainset()\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        # Use SVD algorithm for matrix factorization with reduced complexity for faster training\n",
    "        self.model = SVD(n_factors=20, n_epochs=5, lr_all=0.01, reg_all=0.02)\n",
    "        self.model.fit(trainset)\n",
    "        \n",
    "        print(\"Model training complete!\")\n",
    "        \n",
    "    def get_user_recommendations(self, user_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Get recommendations for a user\n",
    "        \n",
    "        Parameters:\n",
    "        - user_id: The ID of the user to get recommendations for\n",
    "        - top_n: Number of recommendations to return\n",
    "        \n",
    "        Returns:\n",
    "        - List of recommended item IDs with scores\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_mapping:\n",
    "            # If user not in training data, return empty list\n",
    "            return []\n",
    "        \n",
    "        internal_user_id = self.user_mapping[user_id]\n",
    "        \n",
    "        # Get items the user has not interacted with\n",
    "        user_items = set(self.user_items[internal_user_id])\n",
    "        all_items = set(self.reverse_item_mapping.keys())\n",
    "        items_to_predict = list(all_items - user_items)\n",
    "        \n",
    "        # If no items to predict, return empty list\n",
    "        if not items_to_predict:\n",
    "            return []\n",
    "        \n",
    "        # Predict ratings for all items the user has not interacted with\n",
    "        predictions = []\n",
    "        for item_id in items_to_predict:\n",
    "            predicted_rating = self.model.predict(internal_user_id, item_id).est\n",
    "            predictions.append((item_id, predicted_rating))\n",
    "        \n",
    "        # Sort predictions by rating in descending order and take top_n\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_predictions = predictions[:top_n]\n",
    "        \n",
    "        # Convert internal item IDs back to original IDs\n",
    "        recommendations = []\n",
    "        for item_id, score in top_predictions:\n",
    "            original_item_id = self.reverse_item_mapping[item_id]\n",
    "            recommendations.append({\n",
    "                'contentId': str(original_item_id),\n",
    "                'score': float(score),\n",
    "                'reason': f'Based on your reading history (User {user_id})'\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def get_similar_items(self, item_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Get similar items to a given item\n",
    "        \n",
    "        Parameters:\n",
    "        - item_id: The ID of the item to get similar items for\n",
    "        - top_n: Number of similar items to return\n",
    "        \n",
    "        Returns:\n",
    "        - List of similar item IDs with scores\n",
    "        \"\"\"\n",
    "        if item_id not in self.item_mapping:\n",
    "            # If item not in training data, return empty list\n",
    "            return []\n",
    "        \n",
    "        internal_item_id = self.item_mapping[item_id]\n",
    "        \n",
    "        # Get all items except the input item\n",
    "        all_items = set(self.reverse_item_mapping.keys())\n",
    "        all_items.remove(internal_item_id)\n",
    "        \n",
    "        # Calculate similarity between the input item and all other items\n",
    "        similarities = []\n",
    "        for other_item_id in all_items:\n",
    "            # Get users who rated both items\n",
    "            item_users = set(self.item_users[internal_item_id])\n",
    "            other_item_users = set(self.item_users[other_item_id])\n",
    "            common_users = item_users.intersection(other_item_users)\n",
    "            \n",
    "            if not common_users:\n",
    "                continue\n",
    "            \n",
    "            # Calculate similarity based on model factors\n",
    "            item_factors = self.model.qi[internal_item_id]\n",
    "            other_item_factors = self.model.qi[other_item_id]\n",
    "            \n",
    "            # Cosine similarity\n",
    "            similarity = np.dot(item_factors, other_item_factors) / (\n",
    "                np.linalg.norm(item_factors) * np.linalg.norm(other_item_factors)\n",
    "            )\n",
    "            \n",
    "            similarities.append((other_item_id, similarity))\n",
    "        \n",
    "        # Sort similarities in descending order and take top_n\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_similarities = similarities[:top_n]\n",
    "        \n",
    "        # Convert internal item IDs back to original IDs\n",
    "        similar_items = []\n",
    "        for other_item_id, score in top_similarities:\n",
    "            original_item_id = self.reverse_item_mapping[other_item_id]\n",
    "            similar_items.append({\n",
    "                'contentId': str(original_item_id),\n",
    "                'score': float(score),\n",
    "                'reason': f'Similar to article {item_id}'\n",
    "            })\n",
    "        \n",
    "        return similar_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_PATH = '../users_interactions.csv'\n",
    "MODEL_PATH = 'collaborative_model.sav'\n",
    "\n",
    "# Load only necessary columns to save memory\n",
    "interactions_df = pd.read_csv(DATA_PATH, usecols=['personId', 'contentId', 'eventType'])\n",
    "\n",
    "# Display the first few rows\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(interactions_df.isnull().sum())\n",
    "\n",
    "# Remove rows with missing values\n",
    "interactions_df = interactions_df.dropna(subset=['personId', 'contentId', 'eventType'])\n",
    "\n",
    "# Convert IDs to strings to ensure consistent handling\n",
    "interactions_df['personId'] = interactions_df['personId'].astype(str)\n",
    "interactions_df['contentId'] = interactions_df['contentId'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(f\"Total number of interactions: {len(interactions_df)}\")\n",
    "print(f\"Number of unique users: {interactions_df['personId'].nunique()}\")\n",
    "print(f\"Number of unique items: {interactions_df['contentId'].nunique()}\")\n",
    "\n",
    "# Distribution of event types\n",
    "event_counts = interactions_df['eventType'].value_counts()\n",
    "print(\"\\nEvent type distribution:\")\n",
    "print(event_counts)\n",
    "\n",
    "# Visualize event type distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=event_counts.index, y=event_counts.values)\n",
    "plt.title('Distribution of Event Types')\n",
    "plt.xlabel('Event Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data for Faster Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for faster processing\n",
    "print(\"Sampling data for faster processing...\")\n",
    "interactions_df = interactions_df.sample(frac=0.05, random_state=42)\n",
    "print(f\"Sampled {len(interactions_df)} interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model = CollaborativeFilteringModel()\n",
    "model.fit(interactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random user ID from the dataset\n",
    "random_user = interactions_df['personId'].sample(1).iloc[0]\n",
    "print(f\"Getting recommendations for user: {random_user}\")\n",
    "\n",
    "# Get recommendations for the user\n",
    "user_recommendations = model.get_user_recommendations(random_user, top_n=5)\n",
    "print(\"\\nUser recommendations:\")\n",
    "for i, rec in enumerate(user_recommendations):\n",
    "    print(f\"{i+1}. Content ID: {rec['contentId']}, Score: {rec['score']:.4f}, Reason: {rec['reason']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random item ID from the dataset\n",
    "random_item = interactions_df['contentId'].sample(1).iloc[0]\n",
    "print(f\"Getting similar items for item: {random_item}\")\n",
    "\n",
    "# Get similar items\n",
    "similar_items = model.get_similar_items(random_item, top_n=5)\n",
    "print(\"\\nSimilar items:\")\n",
    "for i, item in enumerate(similar_items):\n",
    "    print(f\"{i+1}. Content ID: {item['contentId']}, Score: {item['score']:.4f}, Reason: {item['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print(f\"Saving model to {MODEL_PATH}...\")\n",
    "with open(MODEL_PATH, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully built a collaborative filtering recommendation model using the Surprise library and saved it to a .sav file. This model can be used to provide personalized recommendations to users based on their interaction history, as well as find similar items to a given item."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
