{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-Based Filtering Recommendation Model\n",
    "\n",
    "This notebook builds a content-based filtering recommendation model using text analysis techniques and saves it to a .sav file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Set up visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Content-Based Filtering Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentFilteringModel:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = None\n",
    "        self.content_vectors = None\n",
    "        self.article_ids = None\n",
    "        self.article_titles = None\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess text by removing special characters, converting to lowercase,\n",
    "        removing stopwords, and stemming\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase and remove special characters\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', str(text).lower())\n",
    "        \n",
    "        # Tokenize, remove stopwords, and stem\n",
    "        tokens = [self.stemmer.stem(word) for word in text.split() if word not in self.stop_words]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def fit(self, articles_df):\n",
    "        \"\"\"\n",
    "        Train the content-based filtering model\n",
    "        \n",
    "        Parameters:\n",
    "        - articles_df: DataFrame with columns [contentId, title, text]\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing article content...\")\n",
    "        \n",
    "        # Store article IDs and titles for later use\n",
    "        self.article_ids = articles_df['contentId'].astype(str).tolist()\n",
    "        self.article_titles = articles_df['title'].tolist()\n",
    "        \n",
    "        # Combine title and text for better content representation\n",
    "        # Title is repeated to give it more weight\n",
    "        articles_df['combined_content'] = articles_df['title'].fillna('') + ' ' + \\\n",
    "                                         articles_df['title'].fillna('') + ' ' + \\\n",
    "                                         articles_df['text'].fillna('')\n",
    "        \n",
    "        # Preprocess the combined content\n",
    "        articles_df['processed_content'] = articles_df['combined_content'].apply(self.preprocess_text)\n",
    "        \n",
    "        print(\"Vectorizing article content...\")\n",
    "        # Create TF-IDF vectors\n",
    "        self.vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "        self.content_vectors = self.vectorizer.fit_transform(articles_df['processed_content'])\n",
    "        \n",
    "        print(\"Model training complete!\")\n",
    "        print(f\"Vectorized {self.content_vectors.shape[0]} articles with {self.content_vectors.shape[1]} features\")\n",
    "        \n",
    "    def get_similar_items(self, item_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Get similar items to a given item based on content similarity\n",
    "        \n",
    "        Parameters:\n",
    "        - item_id: The ID of the item to get similar items for\n",
    "        - top_n: Number of similar items to return\n",
    "        \n",
    "        Returns:\n",
    "        - List of similar item IDs with scores\n",
    "        \"\"\"\n",
    "        # Convert item_id to string for consistent handling\n",
    "        item_id = str(item_id)\n",
    "        \n",
    "        # Check if the item exists in our dataset\n",
    "        if item_id not in self.article_ids:\n",
    "            print(f\"Item {item_id} not found in the dataset\")\n",
    "            return []\n",
    "        \n",
    "        # Get the index of the item\n",
    "        item_index = self.article_ids.index(item_id)\n",
    "        \n",
    "        # Get the content vector for the item\n",
    "        item_vector = self.content_vectors[item_index]\n",
    "        \n",
    "        # Calculate similarity with all other items\n",
    "        similarity_scores = cosine_similarity(item_vector, self.content_vectors).flatten()\n",
    "        \n",
    "        # Get indices of top similar items (excluding the item itself)\n",
    "        similar_indices = similarity_scores.argsort()[::-1][1:top_n+1]\n",
    "        \n",
    "        # Create recommendations list\n",
    "        recommendations = []\n",
    "        for idx in similar_indices:\n",
    "            # Get content-based reason\n",
    "            reason = self._get_content_reason(item_index, idx)\n",
    "            \n",
    "            recommendations.append({\n",
    "                'contentId': self.article_ids[idx],\n",
    "                'score': float(similarity_scores[idx]),\n",
    "                'reason': reason\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _get_content_reason(self, item_index, similar_index):\n",
    "        \"\"\"\n",
    "        Generate a content-based reason for the recommendation\n",
    "        \"\"\"\n",
    "        item_title = self.article_titles[item_index] if item_index < len(self.article_titles) else \"this article\"\n",
    "        similar_title = self.article_titles[similar_index] if similar_index < len(self.article_titles) else \"this article\"\n",
    "        \n",
    "        # List of possible content-based reasons\n",
    "        reasons = [\n",
    "            f\"Similar topics to '{item_title}'\",\n",
    "            f\"Similar writing style to '{item_title}'\",\n",
    "            f\"Similar keywords to '{item_title}'\",\n",
    "            f\"Content similar to '{item_title}'\"\n",
    "        ]\n",
    "        \n",
    "        # For simplicity, choose a random reason\n",
    "        # In a real system, you would analyze the specific features that make the items similar\n",
    "        import random\n",
    "        return random.choice(reasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_PATH = '../shared_articles.csv'\n",
    "MODEL_PATH = 'content_model.sav'\n",
    "\n",
    "# Load only necessary columns to save memory\n",
    "articles_df = pd.read_csv(DATA_PATH, usecols=['contentId', 'title', 'text', 'lang'])\n",
    "\n",
    "# Display the first few rows\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(articles_df.isnull().sum())\n",
    "\n",
    "# Filter for English articles only\n",
    "articles_df = articles_df[articles_df['lang'] == 'en']\n",
    "\n",
    "# Remove rows with missing title and text\n",
    "articles_df = articles_df.dropna(subset=['title', 'text'])\n",
    "\n",
    "# Convert IDs to strings to ensure consistent handling\n",
    "articles_df['contentId'] = articles_df['contentId'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(f\"Total number of articles: {len(articles_df)}\")\n",
    "\n",
    "# Distribution of text length\n",
    "articles_df['text_length'] = articles_df['text'].apply(lambda x: len(str(x)))\n",
    "articles_df['title_length'] = articles_df['title'].apply(lambda x: len(str(x)))\n",
    "\n",
    "print(f\"Average title length: {articles_df['title_length'].mean():.2f} characters\")\n",
    "print(f\"Average text length: {articles_df['text_length'].mean():.2f} characters\")\n",
    "\n",
    "# Visualize text length distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(articles_df['text_length'], bins=50)\n",
    "plt.title('Distribution of Article Text Length')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0, articles_df['text_length'].quantile(0.95))  # Limit x-axis to 95th percentile for better visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data for Faster Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for faster processing if needed\n",
    "print(\"Sampling data for faster processing...\")\n",
    "articles_df = articles_df.sample(frac=0.2, random_state=42)\n",
    "print(f\"Sampled {len(articles_df)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model = ContentFilteringModel()\n",
    "model.fit(articles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random article ID from the dataset\n",
    "random_article = articles_df['contentId'].sample(1).iloc[0]\n",
    "print(f\"Getting similar articles for article: {random_article}\")\n",
    "\n",
    "# Get the title of the random article\n",
    "article_title = articles_df[articles_df['contentId'] == random_article]['title'].iloc[0]\n",
    "print(f\"Article title: {article_title}\")\n",
    "\n",
    "# Get similar articles\n",
    "similar_articles = model.get_similar_items(random_article, top_n=5)\n",
    "print(\"\\nSimilar articles:\")\n",
    "for i, article in enumerate(similar_articles):\n",
    "    article_id = article['contentId']\n",
    "    title = articles_df[articles_df['contentId'] == article_id]['title'].iloc[0] if article_id in articles_df['contentId'].values else \"Unknown\"\n",
    "    print(f\"{i+1}. {title}\")\n",
    "    print(f\"   Content ID: {article_id}, Score: {article['score']:.4f}\")\n",
    "    print(f\"   Reason: {article['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most important features (words) in the TF-IDF vectorizer\n",
    "feature_names = np.array(model.vectorizer.get_feature_names_out())\n",
    "\n",
    "# Get the TF-IDF scores for each feature\n",
    "tfidf_scores = np.asarray(model.content_vectors.mean(axis=0)).flatten()\n",
    "\n",
    "# Sort features by TF-IDF score\n",
    "sorted_indices = tfidf_scores.argsort()[::-1]\n",
    "top_features = feature_names[sorted_indices][:20]\n",
    "top_scores = tfidf_scores[sorted_indices][:20]\n",
    "\n",
    "# Visualize top features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=top_scores, y=top_features)\n",
    "plt.title('Top 20 Important Words in the Articles')\n",
    "plt.xlabel('Average TF-IDF Score')\n",
    "plt.ylabel('Words')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print(f\"Saving model to {MODEL_PATH}...\")\n",
    "with open(MODEL_PATH, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully built a content-based filtering recommendation model using text analysis techniques and saved it to a .sav file. This model can be used to find similar articles based on their content, which is particularly useful for recommending items to new users who don't have an interaction history."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
